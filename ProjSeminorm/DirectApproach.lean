/-
Copyright (c) 2026 Tobias Osborne. All rights reserved.
Released under Apache 2.0 license as described in the file LICENSE.
Authors: Tobias Osborne
-/
import ProjSeminorm.Basic

/-!
# Direct Algebraic Approach to Projective Seminorm Multiplicativity

This file documents a direct algebraic attempt to prove
`projectiveSeminorm (v âŠ— w) â‰¥ â€–vâ€– * â€–wâ€–` without using duality (and hence without `h_bidual`).

## The Approach

For a binary tensor product E âŠ— F, given a pure tensor `v âŠ— w = âˆ‘_j v_j âŠ— w_j`:

1. **Reduce to independent case**: Choose a maximal linearly independent subset of `{w_j}`,
   say `w_1, ..., w_s`. The dependent vectors satisfy `w_j = âˆ‘_k a_{jk} w_k` for `j > s`.

2. **Combine terms**: `âˆ‘ v_j âŠ— w_j = âˆ‘_k u_k âŠ— w_k` where `u_k = v_k + âˆ‘_{j>s} a_{jk} v_j`.

3. **Use linear independence**: Since `w_1, ..., w_s` are linearly independent in the tensor
   product and `v âŠ— w = âˆ‘_k u_k âŠ— w_k`, we get `w = âˆ‘_k c_k w_k` and `u_k = c_k v`.

4. **Reduced cost bound**: `âˆ‘_k â€–u_kâ€– Â· â€–w_kâ€– = â€–vâ€– Â· âˆ‘_k |c_k| Â· â€–w_kâ€– â‰¥ â€–vâ€– Â· â€–wâ€–`
   (by the triangle inequality on `w = âˆ‘ c_k w_k`).

## The Obstruction

Step 4 proves the lower bound for the **reduced** representation. But the original
representation has cost `âˆ‘_j â€–v_jâ€– Â· â€–w_jâ€–`, and we need:

  `âˆ‘_j â€–v_jâ€– Â· â€–w_jâ€– â‰¥ âˆ‘_k â€–u_kâ€– Â· â€–w_kâ€–`

This does NOT hold in general! The reduction step changes the cost:

- `u_k = v_k + âˆ‘_{j>s} a_{jk} v_j` gives `â€–u_kâ€– â‰¤ â€–v_kâ€– + âˆ‘_{j>s} |a_{jk}| Â· â€–v_jâ€–`
  (correct direction for bounding `u_k` above, but we need `u_k` below)

- The chain: `â€–vâ€– Â· â€–wâ€– â‰¤ â€–vâ€– Â· âˆ‘_k |c_k| Â· â€–w_kâ€–`   (triangle on w)
            `= âˆ‘_k â€–u_kâ€– Â· â€–w_kâ€–`                       (from u_k = c_k v)
            `â‰¤ âˆ‘_k (âˆ‘_j |a_{jk}| â€–v_jâ€–) Â· â€–w_kâ€–`        (triangle on u_k)
            `= âˆ‘_j â€–v_jâ€– Â· (âˆ‘_k |a_{jk}| â€–w_kâ€–)`        (swap sums)

  We need this last line â‰¤ `âˆ‘_j â€–v_jâ€– Â· â€–w_jâ€–`, i.e.,
  `âˆ‘_k |a_{jk}| Â· â€–w_kâ€– â‰¤ â€–w_jâ€–` for each `j`. But:

  `â€–w_jâ€– = â€–âˆ‘_k a_{jk} w_kâ€– â‰¤ âˆ‘_k |a_{jk}| Â· â€–w_kâ€–`

  This is the WRONG direction! The triangle inequality gives `â‰¤`, but we need `â‰¥`.

## Special Cases

- **Ultrametric fields**: The non-archimedean triangle inequality is nearly tight for
  d-orthogonal bases (defect â‰¤ 1/d), so both inequalities become approximate equalities
  and the proof closes by taking d â†’ 1.

- **Archimedean fields** (â„, â„‚): The triangle inequality can be arbitrarily lossy,
  so this approach is fundamentally stuck. However, for â„/â„‚ the duality approach works
  unconditionally (Hahn-Banach gives isometric bidual embedding).

## Conclusion

The direct algebraic approach fails because reducing a tensor representation to one with
linearly independent components can change the cost in an uncontrollable way. This explains
why the duality-based proof (using `h_bidual`) is the natural route.
-/

open scoped TensorProduct BigOperators

namespace ProjSeminorm.DirectApproach

/-! ### The key algebraic fact

In `E âŠ— F`, if `âˆ‘ e_j âŠ— f_j = 0` and `f_j` are linearly independent (or form a basis),
then `e_j = 0` for all `j`. This is the fact that makes the reduced representation
well-determined.

Mathlib provides `TensorProduct.sum_tmul_basis_right_eq_zero` for the basis version.
-/

-- The key algebraic fact is already in mathlib:
-- TensorProduct.sum_tmul_basis_right_eq_zero:
--   âˆ€ (ğ’ : Basis Îº R N) (b : Îº â†’â‚€ M),
--     b.sum (fun i m => m âŠ—â‚œ ğ’ i) = 0 â†’ b = 0

/-! ### The proof attempt for the reduced case

When `w_1, ..., w_s` are linearly independent and `v âŠ— w = âˆ‘_k u_k âŠ— w_k`,
the coefficients `u_k` are uniquely determined: `u_k = c_k Â· v` where `w = âˆ‘_k c_k w_k`.

For this REDUCED representation, the lower bound holds:
  `âˆ‘_k â€–u_kâ€– Â· â€–w_kâ€– = â€–vâ€– Â· âˆ‘_k |c_k| Â· â€–w_kâ€– â‰¥ â€–vâ€– Â· â€–wâ€–`

This is a straightforward application of the triangle inequality (correct direction). -/

/-- For an independent representation `v âŠ— w = âˆ‘_k (c_k â€¢ v) âŠ— w_k` with
`w = âˆ‘_k c_k â€¢ w_k`, the cost is at least `â€–vâ€– * â€–wâ€–`.

This is the step that DOES work. The obstruction is that arbitrary representations
cannot be reduced to this form without potentially increasing cost. -/
theorem reduced_representation_cost_ge
    {ğ•œ : Type*} [NontriviallyNormedField ğ•œ]
    {E : Type*} [SeminormedAddCommGroup E] [NormedSpace ğ•œ E]
    {F : Type*} [SeminormedAddCommGroup F] [NormedSpace ğ•œ F]
    {n : â„•} (v : E) (c : Fin n â†’ ğ•œ) (w : Fin n â†’ F)
    (w' : F) (hw : âˆ‘ k, c k â€¢ w k = w') :
    â€–vâ€– * â€–w'â€– â‰¤ â€–vâ€– * âˆ‘ k : Fin n, â€–c kâ€– * â€–w kâ€– := by
  gcongr
  calc â€–w'â€– = â€–âˆ‘ k, c k â€¢ w kâ€– := by rw [hw]
    _ â‰¤ âˆ‘ k, â€–c k â€¢ w kâ€– := norm_sum_le ..
    _ = âˆ‘ k, â€–c kâ€– * â€–w kâ€– := by simp_rw [norm_smul]

/-! ### The obstruction

The above theorem shows `â€–vâ€– * â€–wâ€– â‰¤ â€–vâ€– * âˆ‘_k |c_k| * â€–w_kâ€– = âˆ‘_k â€–u_kâ€– * â€–w_kâ€–`
for the REDUCED representation with independent `w_k`.

However, the projective seminorm is the infimum over ALL representations, not just
independent ones. To use the reduced-representation bound, we would need:

  `âˆ‘_j â€–v_jâ€– * â€–w_jâ€– â‰¥ âˆ‘_k â€–u_kâ€– * â€–w_kâ€–`  (original cost â‰¥ reduced cost)

This fails because the reduction `u_k = v_k + âˆ‘_{j>s} a_{jk} v_j` can have
`â€–u_kâ€– > â€–v_kâ€–`, and we lose the cost contributions from the removed terms `j > s`
without compensating for the increased cost of the remaining terms.

The following shows the wrong-direction inequality explicitly. -/

/-- The triangle inequality goes the wrong way for the reduction step.
Given `w_j = âˆ‘_k a_{jk} w_k` (expressing dependent vectors in terms of independent ones),
we have `âˆ‘_k |a_{jk}| * â€–w_kâ€– â‰¥ â€–w_jâ€–`, NOT `â‰¤`. So the swap-of-sums argument
produces a quantity LARGER than both `â€–vâ€– * â€–wâ€–` and `âˆ‘_j â€–v_jâ€– * â€–w_jâ€–`. -/
theorem triangle_wrong_direction
    {ğ•œ : Type*} [NontriviallyNormedField ğ•œ]
    {F : Type*} [SeminormedAddCommGroup F] [NormedSpace ğ•œ F]
    {n : â„•} (a : Fin n â†’ ğ•œ) (w : Fin n â†’ F) :
    â€–âˆ‘ k, a k â€¢ w kâ€– â‰¤ âˆ‘ k, â€–a kâ€– * â€–w kâ€– := by
  calc â€–âˆ‘ k, a k â€¢ w kâ€– â‰¤ âˆ‘ k, â€–a k â€¢ w kâ€– := norm_sum_le ..
    _ = âˆ‘ k, â€–a kâ€– * â€–w kâ€– := by simp_rw [norm_smul]

-- The above gives `â€–w_jâ€– â‰¤ âˆ‘_k |a_{jk}| * â€–w_kâ€–` â€” correct, but useless.
-- We NEED `â€–w_jâ€– â‰¥ âˆ‘_k |a_{jk}| * â€–w_kâ€–` to close the proof, which is false in general.

end ProjSeminorm.DirectApproach
